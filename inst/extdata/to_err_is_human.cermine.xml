<?xml version="1.0" encoding="UTF-8"?>
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta/>
    <article-meta>
      <title-group>
        <article-title>To Err is Human: An Empirical Investigation</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Daniel Lakens</string-name>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Lisa DeBruine</string-name>
        </contrib>
      </contrib-group>
      <abstract>
        <p>This paper demonstrates some good and poor practices for use with the {metacheck} R package and Shiny app. All data are simulated. The paper shows examples of (1) open and closed OSF links; (2) citation of retracted papers; (3) missing/mismatched citations and references; (4) imprecise reporting of p-values; and (5) use of “marginally significant” to describe non-significant findings. Although intentional dishonestly might be a successful way to boost creativity (Gino &amp; Wiltermuth, 2014), it is safe to say most mistakes researchers make are unintentional. From a human factors perspective, human error is a symptom of a poor design (Smithy, 2020). Automation can be use to check for errors in scientific manuscripts, and inform authors about possible corrections. In this study we examine the usefulness of metacheck to improve best practices.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>Introduction</title>
      <p>vars
mistakes
usefulness
experience
control
experimental
condition</p>
    </sec>
    <sec id="sec-2">
      <title>Results</title>
      <p>All data needed to reproduce the analyses in Table 1 is available from https://osf.io/5tbm9
and code is available from the OSF.
On average researchers in the experimental (app) condition made fewer mistakes (M = 9.12)
than researchers in the control (checklist) condition (M = 10.9), t(97.7) = 2.9, p = 0.005.
On average researchers in the experimental condition found the app marginally significantly
more useful (M = 5.06) than researchers in the control condition found the checklist (M =
4.5), t(97.2) = -1.96, p = 0.152.</p>
      <p>Scatter Plot with Correlation: 0.04</p>
      <p>5</p>
    </sec>
    <sec id="sec-3">
      <title>Discussion References</title>
      <p>6
9
12</p>
      <p>15</p>
      <p>Mistakes
There was no efect of experience on the reduction in errors when using the tool ( p &gt; .05), as
the correlation was non-significant.</p>
      <p>It seems automated tools can help prevent errors by providing researchers with feedback about
potential mistakes, and researchers feel the app is useful. We conclude the use of automated
checks has potential to reduce the number of mistakes in scientific manuscripts.
Smith, F. (2021). Human error is a symptom of a poor design. Journal of Journals, 0(0), 0.
https://doi.org/10.0000/0123456789</p>
    </sec>
  </body>
  <back>
    <ref-list/>
  </back>
</article>
